# ğŸ“º TV Show RAG Recommender System

Este proyecto implementa un sistema de recomendaciÃ³n semÃ¡ntica de series de televisiÃ³n ("TV Shows") utilizando una arquitectura **RAG (Retrieval-Augmented Generation)** personalizada.

Combina datos de mÃºltiples plataformas de streaming (Netflix, Amazon Prime, Hulu, Disney+), los procesa mediante embeddings locales (**Ollama**) y los almacena en un grafo (**Neo4j**). La inferencia final se realiza con **Groq** para obtener respuestas ultrarrÃ¡pidas.

## ğŸ—ï¸ Arquitectura TÃ©cnica

El sistema no utiliza un Ã­ndice vectorial estÃ¡ndar de "caja negra", sino una implementaciÃ³n hÃ­brida:

1.  **ETL & Preprocesamiento:**
    * FusiÃ³n de 4 datasets CSV.
    * NormalizaciÃ³n de columnas y limpieza de nulos (`dropna`, `drop_duplicates`).
    * **Estrategia de Embedding:** Se concatena `title` + `description` + `genre` para generar el vector.
2.  **Almacenamiento (Neo4j):**
    * Los datos se guardan como nodos con la etiqueta `:TVShow`.
3.  **Motor de BÃºsqueda (Custom Retrieval):**
    * Se extraen los candidatos de Neo4j.
    * Se calcula la **Similitud del Coseno** en memoria utilizando `NumPy` entre el query del usuario y los vectores almacenados.
4.  **GeneraciÃ³n (Groq):**
    * Se inyectan los Top-K (5) resultados mÃ¡s similares en el prompt del sistema.
    * Modelo LLM: `llama-3.1-8b-instant`.

---

## ğŸ› ï¸ Requisitos e InstalaciÃ³n

### 1. Prerrequisitos de Infraestructura
* **Python 3.9+**
* **Neo4j Database:** Instancia activa (local o Docker).
* **Ollama:** EjecutÃ¡ndose localmente para embeddings.
* **Groq API Key:** Necesaria para el LLM.

### 2. Dependencias Python
Instala las librerÃ­as utilizadas en el notebook:

```bash
pip install pandas numpy
pip install neo4j
pip install fastapi uvicorn
pip install llama-index-core
pip install llama-index-llms-groq
pip install llama-index-embeddings-ollama
```

### 3. ConfiguraciÃ³n de Modelos
El sistema espera el siguiente modelo de embeddings en Ollama:

```bash
ollama pull mxbai-embed-large
```

---

## ğŸ“‚ Estructura de Datos y Archivos

Para que el pipeline de ingestiÃ³n funcione, la estructura de carpetas debe ser:

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ netflix_titles.csv
â”‚   â”œâ”€â”€ amazon_prime_titles.csv
â”‚   â”œâ”€â”€ hulu_titles.csv
â”‚   â”œâ”€â”€ disney_plus_titles.csv
â”‚   â””â”€â”€ clean/                  # Generado automÃ¡ticamente por el script
â”œâ”€â”€ front/                      # Archivos estÃ¡ticos del frontend
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ main.py                 # LÃ³gica de FastAPI
â”‚   â””â”€â”€ groq_recommender.py     # Clase GroqRecommender
â””â”€â”€ script.ipynb                # Notebook de ingestiÃ³n y pruebas
```

---

## ğŸ’¾ Esquema de Base de Datos (Neo4j)

El script de ingestiÃ³n crea nodos con la siguiente estructura Cypher:

```cypher
(:TVShow {
    title: String,
    description: String,
    release_year: Integer,
    genre: String,
    platform: String,
    embedding: List<Float>
})
```

---

## ğŸš€ EjecuciÃ³n y Uso

### Paso 1: Ingesta de Datos
Ejecuta las celdas del notebook `script.ipynb` o el script de ETL derivado para:
1.  Limpiar y unificar CSVs.
2.  Generar embeddings (esto puede tomar tiempo dependiendo del hardware).
3.  Cargar los datos en Neo4j.

### Paso 2: Levantar el Servidor API
El servidor utiliza `uvicorn` y `FastAPI`.

```bash
# AsegÃºrate de estar en la raÃ­z del proyecto
uvicorn scripts.main:app --host 127.0.0.1 --port 8000 --reload
```

### Paso 3: Consumo de la API (Polling)
La API es asÃ­ncrona para no bloquear el hilo principal mientras el LLM piensa.

**1. Enviar consulta (`POST /send`):**
```json
{
  "content": "RecomiÃ©ndame algo de terror psicolÃ³gico de los 90"
}
```
**Respuesta:**
```json
{
  "link": "/result/a1b2c3d4"
}
```

**2. Obtener resultado (`GET /result/a1b2c3d4`):**
* Si estÃ¡ procesando: `{"status": "not ready"}`
* Si terminÃ³:
```json
{
  "status": "ready",
  "message": "Te recomiendo ver Twin Peaks..."
}
```

---
**Nota:** AsegÃºrate de configurar las variables de entorno o editar las constantes `NEO4J_URI`, `NEO4J_PASSWORD` y `groq_api_key` en el archivo `groq_recommender.py` antes de iniciar.
